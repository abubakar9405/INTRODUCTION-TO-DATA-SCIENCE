{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Wikipedia\n"
     ]
    }
   ],
   "source": [
    "#TASK-01\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url=('https://www.wikipedia.org/')\n",
    "page=requests.get(url)\n",
    "print(page)#200 means it returns the requested content successfully\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "print(soup.title.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "日本語\n",
      "Русский\n",
      "Español\n",
      "Deutsch\n",
      "Français\n",
      "Italiano\n",
      "中文\n",
      "Português\n",
      "فارسی\n"
     ]
    }
   ],
   "source": [
    "#TASK-02\n",
    "\n",
    "languagess=soup.find_all('nav',class_='central-featured')[0].find_all('strong')\n",
    "for i in languagess:\n",
    " print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//en.wikipedia.org/\n",
      "//ja.wikipedia.org/\n",
      "//ru.wikipedia.org/\n",
      "//es.wikipedia.org/\n",
      "//de.wikipedia.org/\n",
      "//fr.wikipedia.org/\n",
      "//it.wikipedia.org/\n",
      "//zh.wikipedia.org/\n",
      "//pt.wikipedia.org/\n",
      "//fa.wikipedia.org/\n"
     ]
    }
   ],
   "source": [
    "#TASK-03\n",
    "#printing URLS OF ALL LISTED LANGUAGES\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "URL=requests.get('https://www.wikipedia.org/')\n",
    "BS=BeautifulSoup(URL.content,'html.parser')\n",
    "links=BS.findAll('a',class_='link-box')#links for language boxes...\n",
    "for i in links:\n",
    "    href=i.get('href')#getting URL'S\n",
    "    if href:\n",
    "        print(href) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK-04\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "URL=requests.get('https://www.wikipedia.org/')\n",
    "print(URL)\n",
    "BS=BeautifulSoup(URL.content,'html.parser')\n",
    "p=BS.find_all('p')\n",
    "count=0\n",
    "for i in p:\n",
    "    count=count+1\n",
    "\n",
    "print(count,'\\n')    \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featured Article Title: The Riddle of the Sphinx\n"
     ]
    }
   ],
   "source": [
    "#TASK-05\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "URL = requests.get('https://en.wikipedia.org/wiki/Main_Page')#english homepage\n",
    "\n",
    "# Parse the content with BeautifulSoup\n",
    "BS = BeautifulSoup(URL.content, 'html.parser')\n",
    "F_A = BS.find('div', id='mp-tfa')#featured article is under articl mp-tfa\n",
    "if F_A:  # Ensure the section exists\n",
    "    F_A_T = F_A.find('b').text  #TAG B HAS TITLE...\n",
    "    print(\"Featured Article Title:\", F_A_T)\n",
    "else:\n",
    "    print(\"Featured Article section not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL found: https://meta.wikimedia.org/wiki/Special:MyLanguage/List_of_Wikipedias\n"
     ]
    }
   ],
   "source": [
    "#TASK-06\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "INPUT_URL = input(\"Enter the URL (or part of it): \")\n",
    "URL = requests.get('https://www.wikipedia.org/')\n",
    "BS = BeautifulSoup(URL.content, 'html.parser')\n",
    "url_found = False\n",
    "LINK = BS.find_all('a')\n",
    "\n",
    "for i in LINK:\n",
    "    href = i.get('href')  # Get the href (URL) from the <a> tag\n",
    "    \n",
    "    if href and INPUT_URL in href:\n",
    "        print(\"URL found:\", href)\n",
    "        url_found = True\n",
    "        break \n",
    "\n",
    "# If the URL was not found, print \"invalid URL\"\n",
    "if not url_found:\n",
    "    print(\"Invalid URL\")\n",
    "    # SAMPLE URLS\n",
    "    #Help:\n",
    "    #wikipedia.org\n",
    "    #wiki/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the news:\n",
      "Brighton hotel bombing\n",
      "Tomorrow Speculative Fiction\n",
      "To Kill a Mockingbird\n"
     ]
    }
   ],
   "source": [
    "#TASK-07\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "response = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "BS = BeautifulSoup(response.content, 'html.parser')\n",
    "news_section = BS.find('div', id='mp-upper')\n",
    "if news_section:\n",
    "    news_items = news_section.find('ul').find_all('li') \n",
    "    print(\"In the news:\")\n",
    "    for item in news_items:\n",
    "        print(item.string)\n",
    "else:\n",
    "    print(\"News section not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you know?\n",
      "... that Santa opened the 2024 Summer Paralympics closing ceremony (pictured)?\n",
      "... that researchers speculate that cocoa butter forms crystals on sugar while chocolate is tempered?\n",
      "... that the noken system used for voting in parts of Indonesia can produce turnouts of 100%?\n",
      "... that for 19 years Tonya Burns had the only retired jersey number in Iowa State women's basketball history?\n",
      "... that Northamptonshire has had ninety-two railway stations, but now has only six?\n",
      "... that Joe Wirkkunen coached the Finland men's national ice hockey team after receiving a recommendation from Canada?\n",
      "... that the music video for \"...Well, Better Than the Alternative\" uses software to render handmade paintings over live-action footage of musician Will Wood?\n",
      "... that British conscientious objector Henry Firth died in 1918 while being held at a work camp in Dartmoor?\n",
      "... that the ongoing premiere of Gregory Markopoulos's Eniaios started 20 years ago?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Fetch the content of the Wikipedia main page\n",
    "response = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "BS = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the 'Did you know?' section\n",
    "D_Y_K = BS.find('div', id='mp-dyk')  # Use id instead of class\n",
    "\n",
    "if D_Y_K:\n",
    "    DYK_items = D_Y_K.find('ul').find_all('li')\n",
    "    print(\"Did you know?\")\n",
    "    for item in DYK_items:\n",
    "        print(item.text)\n",
    "else:\n",
    "    print(\"Did you know? section not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
